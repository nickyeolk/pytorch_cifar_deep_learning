initiate model, device is,  cuda:0
convert
model initiated
[1,   500] loss: 1.118
[1,  1000] loss: 0.747
[2,   500] loss: 0.684
[2,  1000] loss: 0.659
[3,   500] loss: 0.639
[3,  1000] loss: 0.624
[4,   500] loss: 0.605
[4,  1000] loss: 0.616
[5,   500] loss: 0.598
[5,  1000] loss: 0.594
[6,   500] loss: 0.596
[6,  1000] loss: 0.578
[7,   500] loss: 0.579
[7,  1000] loss: 0.584
[8,   500] loss: 0.579
[8,  1000] loss: 0.566
[9,   500] loss: 0.572
[9,  1000] loss: 0.565
[10,   500] loss: 0.562
[10,  1000] loss: 0.572
[11,   500] loss: 0.564
[11,  1000] loss: 0.557
[12,   500] loss: 0.562
[12,  1000] loss: 0.555
[13,   500] loss: 0.553
[13,  1000] loss: 0.560
[14,   500] loss: 0.548
[14,  1000] loss: 0.554
[15,   500] loss: 0.552
[15,  1000] loss: 0.554
[16,   500] loss: 0.546
[16,  1000] loss: 0.551
[17,   500] loss: 0.544
[17,  1000] loss: 0.552
[18,   500] loss: 0.544
[18,  1000] loss: 0.546
[19,   500] loss: 0.549
[19,  1000] loss: 0.540
[20,   500] loss: 0.537
[20,  1000] loss: 0.550
[21,   500] loss: 0.528
[21,  1000] loss: 0.553
[22,   500] loss: 0.537
[22,  1000] loss: 0.542
[23,   500] loss: 0.546
[23,  1000] loss: 0.534
[24,   500] loss: 0.535
[24,  1000] loss: 0.544
[25,   500] loss: 0.531
[25,  1000] loss: 0.543
[26,   500] loss: 0.535
[26,  1000] loss: 0.538
[27,   500] loss: 0.539
[27,  1000] loss: 0.532
[28,   500] loss: 0.526
[28,  1000] loss: 0.542
[29,   500] loss: 0.536
[29,  1000] loss: 0.533
[30,   500] loss: 0.527
[30,  1000] loss: 0.541
[31,   500] loss: 0.531
[31,  1000] loss: 0.530
[32,   500] loss: 0.525
[32,  1000] loss: 0.535
[33,   500] loss: 0.525
[33,  1000] loss: 0.540
[34,   500] loss: 0.534
[34,  1000] loss: 0.520
[35,   500] loss: 0.527
[35,  1000] loss: 0.531
[36,   500] loss: 0.535
[36,  1000] loss: 0.529
[37,   500] loss: 0.528
[37,  1000] loss: 0.533
[38,   500] loss: 0.534
[38,  1000] loss: 0.525
[39,   500] loss: 0.527
[39,  1000] loss: 0.531
[40,   500] loss: 0.518
[40,  1000] loss: 0.535
[41,   500] loss: 0.520
[41,  1000] loss: 0.538
[42,   500] loss: 0.528
[42,  1000] loss: 0.530
[43,   500] loss: 0.523
[43,  1000] loss: 0.532
[44,   500] loss: 0.526
[44,  1000] loss: 0.530
[45,   500] loss: 0.527
[45,  1000] loss: 0.523
[46,   500] loss: 0.522
[46,  1000] loss: 0.530
[47,   500] loss: 0.522
[47,  1000] loss: 0.526
[48,   500] loss: 0.522
[48,  1000] loss: 0.525
[49,   500] loss: 0.532
[49,  1000] loss: 0.524
[50,   500] loss: 0.516
[50,  1000] loss: 0.532
[51,   500] loss: 0.529
[51,  1000] loss: 0.520
[52,   500] loss: 0.527
[52,  1000] loss: 0.521
[53,   500] loss: 0.517
[53,  1000] loss: 0.525
[54,   500] loss: 0.515
[54,  1000] loss: 0.530
[55,   500] loss: 0.521
[55,  1000] loss: 0.522
[56,   500] loss: 0.522
[56,  1000] loss: 0.522
[57,   500] loss: 0.526
[57,  1000] loss: 0.512
[58,   500] loss: 0.522
[58,  1000] loss: 0.525
[59,   500] loss: 0.520
[59,  1000] loss: 0.520
[60,   500] loss: 0.521
[60,  1000] loss: 0.526
[61,   500] loss: 0.517
[61,  1000] loss: 0.526
[62,   500] loss: 0.519
[62,  1000] loss: 0.521
[63,   500] loss: 0.523
[63,  1000] loss: 0.523
[64,   500] loss: 0.519
[64,  1000] loss: 0.523
[65,   500] loss: 0.517
[65,  1000] loss: 0.523
[66,   500] loss: 0.519
[66,  1000] loss: 0.521
[67,   500] loss: 0.519
[67,  1000] loss: 0.519
[68,   500] loss: 0.529
[68,  1000] loss: 0.509
[69,   500] loss: 0.514
[69,  1000] loss: 0.525
[70,   500] loss: 0.516
[70,  1000] loss: 0.520
[71,   500] loss: 0.518
[71,  1000] loss: 0.523
[72,   500] loss: 0.522
[72,  1000] loss: 0.518
[73,   500] loss: 0.517
[73,  1000] loss: 0.519
[74,   500] loss: 0.510
[74,  1000] loss: 0.527
[75,   500] loss: 0.523
[75,  1000] loss: 0.511
[76,   500] loss: 0.518
[76,  1000] loss: 0.517
[77,   500] loss: 0.520
[77,  1000] loss: 0.517
[78,   500] loss: 0.508
[78,  1000] loss: 0.527
[79,   500] loss: 0.517
[79,  1000] loss: 0.515
[80,   500] loss: 0.509
[80,  1000] loss: 0.528
[81,   500] loss: 0.516
[81,  1000] loss: 0.518
[82,   500] loss: 0.521
[82,  1000] loss: 0.517
[83,   500] loss: 0.518
[83,  1000] loss: 0.522
[84,   500] loss: 0.515
[84,  1000] loss: 0.522
[85,   500] loss: 0.519
[85,  1000] loss: 0.509
[86,   500] loss: 0.510
[86,  1000] loss: 0.525
[87,   500] loss: 0.520
[87,  1000] loss: 0.515
[88,   500] loss: 0.511
[88,  1000] loss: 0.521
[89,   500] loss: 0.517
[89,  1000] loss: 0.515
[90,   500] loss: 0.502
[90,  1000] loss: 0.530
[91,   500] loss: 0.509
[91,  1000] loss: 0.521
[92,   500] loss: 0.516
[92,  1000] loss: 0.511
[93,   500] loss: 0.512
[93,  1000] loss: 0.521
[94,   500] loss: 0.521
[94,  1000] loss: 0.524
[95,   500] loss: 0.513
[95,  1000] loss: 0.520
[96,   500] loss: 0.518
[96,  1000] loss: 0.512
[97,   500] loss: 0.518
[97,  1000] loss: 0.512
[98,   500] loss: 0.517
[98,  1000] loss: 0.516
[99,   500] loss: 0.509
[99,  1000] loss: 0.525
[100,   500] loss: 0.521
[100,  1000] loss: 0.511
Finished Training
training complete <_io.TextIOWrapper name='transfer_learning_output.txt' mode='a' encoding='UTF-8'>
